\section{Related Works}
\label{sec:related_works}

Cross-view geo-localization addresses the fundamental challenge of matching images captured from drastically different viewpoints of the same geographic location. This task has evolved from traditional handcrafted feature-based approaches to sophisticated deep learning methods, driven by applications in robotics, augmented reality, and autonomous navigation.

\subsection{Cross-View Geo-localization and Orientation Estimation}

Early pioneering works by Workman and Jacobs~\cite{workman2015predicting,workman2015learning} established the foundation for cross-view matching by demonstrating the potential of CNNs for learning robust feature representations across viewpoint variations. The introduction of benchmark datasets like CVUSA~\cite{workman2015predicting} and later VIGOR~\cite{zhu2021vigor} catalyzed systematic research in this domain, with the latter providing more challenging same-area and cross-area evaluation protocols that better reflect real-world deployment scenarios.

Subsequent developments focused on addressing the inherent domain gap between aerial and ground imagery. Notable approaches include CVM-Net~\cite{hu2018cvm}, which introduced dual-stream CNN architectures with polar transformations to align spatial layouts, and SAFA~\cite{NEURIPS2019_ba2f0015}, which employed attention mechanisms for spatial-aware feature aggregation. Methods like CVFT~\cite{shi2019optimalfeaturetransportcrossview} tackled the domain gap through feature transport modules, while others explored generative approaches to synthesize cross-view correspondences~\cite{regmi2018cross}.

The recognition that orientation estimation is crucial for disambiguation and practical applications led to joint location and orientation frameworks. Recent works have emphasized the importance of spatial awareness in feature representations~\cite{shi2020ilookingatjoint}, particularly for applications requiring precise alignment such as outdoor augmented reality. However, many existing methods either treat orientation as a byproduct of location retrieval or require extensive supervised training with orientation labels.

\subsection{Vision Transformers and Self-Supervised Learning}

The advent of Vision Transformers (ViTs)~\cite{dosovitskiy2020image} has revolutionized cross-view geo-localization by enabling models to capture global spatial relationships through self-attention mechanisms. Transformer-based approaches like L2LTR~\cite{yang2021cross} and TransGeo~\cite{wang2021multi} have demonstrated superior performance over CNN-based methods, leveraging learnable position encodings and global context modeling.

Concurrently, self-supervised learning has emerged as a powerful paradigm for learning robust visual representations without manual annotations. Methods such as MoCo~\cite{he2020momentum}, SimCLR~\cite{chen2020simple}, and BYOL~\cite{grill2020bootstrap} have shown that self-supervised features can match or exceed supervised counterparts on various downstream tasks. DINOv2~\cite{oquab2023dinov2} represents the current state-of-the-art, providing features particularly well-suited for dense prediction tasks and fine-grained visual understanding.

Despite these advances, most transformer-based cross-view methods still rely heavily on supervised learning with orientation labels, requiring extensive annotation efforts. Recent works have begun exploring self-supervised features for geo-localization tasks, but typically treat cross-view matching as standard retrieval without considering the specific geometric constraints and orientation relationships inherent in cross-view scenarios.

\subsection{Multi-Modal Integration and Dataset Limitations}

Recent research has recognized the value of incorporating complementary modalities to improve cross-view matching performance. Sky segmentation techniques help eliminate uninformative regions in ground-level images, focusing attention on building structures and terrain features visible in aerial views~\cite{workman2015predicting}. Advanced monocular depth estimation models like Depth-Anything~\cite{yang2024depth} provide robust geometric cues that enable multi-scale feature aggregation and informed spatial reasoning.

Attention mechanisms have proven particularly effective for cross-view alignment, with cross-attention layers learning to focus on corresponding regions between aerial and ground views. However, most attention-based methods require supervised training with orientation labels and struggle with the limited field-of-view constraints common in real-world applications.

Current benchmark datasets, while valuable, present limitations for comprehensive evaluation. CVUSA focuses primarily on the United States, while VIGOR, though more diverse, still covers a limited geographical scope. These datasets primarily support geo-localization and retrieval tasks, with growing interest in orientation estimation as a distinct but related problem. The lack of large-scale, geographically diverse datasets with comprehensive global coverage has hindered the development of truly robust cross-view methods that generalize across different environmental conditions and cultural contexts.

Our work addresses these limitations by introducing CVGlobal, a large-scale dataset with balanced global representation, and proposing novel orientation-aware methods that combine self-supervised features with multi-modal cues for robust cross-view orientation estimation without requiring extensive supervision.