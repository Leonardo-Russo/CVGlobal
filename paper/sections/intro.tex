\section{Introduction}
\label{sec:intro}

Image alignment is a fundamental task in computer vision with wide-ranging applications including image stitching, 3D reconstruction, and augmented reality. Traditional approaches for image alignment often rely on handcrafted feature detectors and descriptors, followed by robust matching techniques. While effective in many scenarios, these methods can struggle when faced with significant viewpoint changes or complex scenes. 

The recent advancements in deep learning, particularly with Vision Transformer (ViT) models, have opened new avenues for addressing challenges in image alignment. ViTs have demonstrated exceptional capabilities in learning rich and discriminative visual representations, enabling them to capture semantic information effectively. This has led to significant progress in various computer vision tasks, including image classification, object detection, and semantic segmentation. 

In this work, we explore the potential of pre-trained ViT models for aligning ground-view and aerial-view images, a challenging task due to the drastic difference in perspectives. We leverage the powerful representations learned by these models to establish correspondences between salient elements in both views. Our approach builds upon the hypothesis that similar objects or regions, such as roads, buildings, or vegetation, should exhibit similar embeddings or tokens in the model's output space, even when observed from vastly different viewpoints.

To enhance the alignment accuracy, we incorporate sky filtering and depth estimation techniques to eliminate irrelevant sky regions and prioritize ground features. We also introduce a novel alignment strategy based on averaging tokens along vertical lines in the ground image and radial lines in the aerial image, enabling robust comparison of representations across the two views. 

Our experimental results on the CVUSA dataset demonstrate the effectiveness of our proposed approach. We achieve promising alignment accuracy, even in the presence of challenging scenarios with significant viewpoint changes and complex scenes. 

The main contributions of this paper can be summarized as follows:

\begin{itemize}
    \item We propose a novel approach for aligning ground-view and aerial-view images using pre-trained ViT models.
    \item We introduce a robust alignment strategy based on averaging tokens along vertical and radial lines, enabling effective comparison of representations across the two views.
    \item We incorporate sky filtering and depth estimation techniques to improve alignment accuracy by focusing on relevant ground features.
    \item Our experimental results on the CVUSA dataset demonstrate the effectiveness of our proposed approach.
\end{itemize}